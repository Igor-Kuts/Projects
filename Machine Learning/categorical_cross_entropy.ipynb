{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80481064-08e8-4fb2-8c2f-cfe2848d0d99",
   "metadata": {},
   "source": [
    "## Categorical Cross-Entropy Loss\n",
    "\n",
    "**Categorical cross-entropy** is a loss function commonly used in machine learning and deep learning algorithms for <u>multi-class classification problems</u>. \n",
    "It measures the dissimilarity between the predicted probability distribution and the true probability distribution of the classes.  \n",
    "\n",
    "Categorical cross-entropy is often employed when the classes are mutually exclusive, meaning *each input belongs to only one class*.\n",
    "\n",
    "The formula for categorical cross-entropy is as follows:\n",
    "\n",
    "$$\n",
    "CategoricalCrossEntropy(\\mathbf{y, p}) = -\\sum_{i=1}^{n} y_i \\cdot \\log(p_i)\n",
    "$$\n",
    "\n",
    "In this formula, $\\mathbf{y} = [y_1, y_2, \\ldots, y_n]$ represents the true probability distribution of the classes, and $\\mathbf{p} = [p_1, p_2, \\ldots, p_n]$ represents the predicted probability distribution, which are usually the **result of output layer with softmax function**. The summation symbol $\\sum$ denotes the sum over all classes, and $\\log$ represents the natural logarithm. The negative sign at the beginning indicates that it is a minimization problem, aiming to minimize the dissimilarity between the predicted distribution $\\mathbf{p}$ and the true distribution $\\mathbf{y}$.\n",
    "\n",
    "### Other aliases of categorical cross-entropy\n",
    "\n",
    "Categorical cross-entropy is commonly referred to by several other names in the literature and different fields. Some of the alternative names for categorical cross-entropy include:\n",
    "\n",
    "- Log loss\n",
    "- Multiclass cross-entropy\n",
    "- Softmax cross-entropy\n",
    "- Negative log-likelihood loss\n",
    "- Cross-entropy loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Application\n",
    "\n",
    "When **training a neural network**, the goal is to <u>minimize the categorical cross-entropy loss</u>. This is typically done by using gradient-based optimization algorithms, such as stochastic gradient descent (**SGD**), gradient descent (**GD**), adaptive moment estimation (**ADAM**)  or **Sophia** (which is a new optimization algorithm, 2023) to update the network's parameters iteratively.\n",
    "\n",
    "\n",
    "\n",
    "### Additional insights on Categorical Cross-Entropy Loss:\n",
    "\n",
    "1. **One-Hot Encoding**: Categorical cross-entropy loss requires the true class labels to be one-hot encoded, where each class is represented as a binary vector with a single '1' and '0's elsewhere. \n",
    "\n",
    "\n",
    "2. **Overfitting Detection**: Increasing categorical cross-entropy loss on the validation set while the training loss continues to decrease may indicate overfitting, warranting the need for regularization or early stopping.\n",
    "\n",
    "3. **Normalization**: Predicted probabilities should be normalized using softmax.\n",
    "\n",
    "4. **Label Smoothing**: To improve model robustness, label smoothing is sometimes applied by replacing the '1' in the one-hot encoded labels with a value slightly less than 1 and distributing the remaining value across other classes.\n",
    "\n",
    "5. **Class Imbalance Handling**: In scenarios where the class distribution is imbalanced, meaning some classes have significantly more or fewer samples than others, it is beneficial to consider techniques such as class weighting or data augmentation to address the impact of imbalanced classes on the categorical cross-entropy loss.\n",
    "\n",
    "6. **Early Stopping**: Categorical cross-entropy loss can be used as a criterion for early stopping during training. If the loss on a validation set starts to increase consistently after a certain number of epochs, training can be halted to prevent overfitting and select the best-performing model.\n",
    "\n",
    "7. **Information Gain**: Categorical cross-entropy loss can be interpreted as the measure of information gain or reduction in uncertainty about the true class given the predicted probabilities. Minimizing the loss encourages the model to maximize the information gain and improve its predictive power.\n",
    "\n",
    "These insights provide a solid foundation for utilizing Categorical Cross-Entropy loss effectively in various machine learning tasks, particularly in multi-class classification problems.\n",
    "\n",
    "**Below we will compute Categorical Cross-Entropy Loss via 2 different methods: PyTroch and Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde651a3-0949-4ee8-8e70-79ebf090911c",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ca18547-7470-4831-b270-f486bf6c68f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70697fa9-38e2-4741-967a-7476e854a653",
   "metadata": {},
   "source": [
    "### Compute Categorical Cross-Entropy with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0547d825-d639-4e38-848c-2ed921b33514",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Cross-Entropy Loss with PyTorch: 0.9714786410331726\n"
     ]
    }
   ],
   "source": [
    "# Define the ground truth labels and predicted logits\n",
    "true_labels_torch = torch.tensor([1, 0, 2])  # Ground truth labels (indices)\n",
    "\n",
    "predicted_logits_torch = torch.tensor([[0.4, 1.5, 0.8],\n",
    "                                       [0.3, 0.9, 0.2],\n",
    "                                       [0.6, 0.1, 1.5]])  # Predicted logits\n",
    "\n",
    "# Apply softmax to convert logits into probabilities\n",
    "predicted_probs_torch = F.softmax(predicted_logits_torch, dim=1)\n",
    "\n",
    "# Compute the categorical cross-entropy loss\n",
    "loss = F.cross_entropy(predicted_probs_torch, true_labels_torch)\n",
    "\n",
    "print('Categorical Cross-Entropy Loss with PyTorch:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8741e67-4e47-4dba-b3d0-9139b9787260",
   "metadata": {},
   "source": [
    "### Compute Categorical Cross-Entropy with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b14eebb-3d09-4ae5-9480-d814375141e0",
   "metadata": {},
   "source": [
    "##### Custom functions for softmax and Categorical Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55c17f84-2a50-47ea-a7df-7dd7dd9be4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax function for an array of logits.\n",
    "    \n",
    "    Args:\n",
    "        x (numpy.ndarray): Input array of logits.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Array of softmax probabilities.\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "921aa614-eba0-436a-b416-d240b2bbf054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def categorical_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute categorical cross-entropy loss between y_true and y_pred.\n",
    "    \n",
    "    Args:\n",
    "        y_true (numpy.ndarray): True labels in index format. Shape (batch_size,).\n",
    "        y_pred (numpy.ndarray): Predicted probabilities for each class. Shape (batch_size, num_classes).\n",
    "    \n",
    "    Returns:\n",
    "        float: Categorical cross-entropy loss.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-10  # small value to avoid division by zero\n",
    "    \n",
    "    # Convert true labels to one-hot encoded format\n",
    "    num_classes = y_pred.shape[1]\n",
    "    y_true_one_hot = np.eye(num_classes)[y_true]\n",
    "    \n",
    "    # Clip predicted values to prevent numerical instability\n",
    "    y_pred = np.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "    \n",
    "    # Compute cross-entropy loss\n",
    "    loss = -np.sum(y_true_one_hot * np.log(y_pred + epsilon)) / y_true.shape[0]\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c156a822-5eca-473e-b8c3-b215e30dcbdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Cross-Entropy Loss with Numpy: 0.8074344512710289\n"
     ]
    }
   ],
   "source": [
    "# Define the ground truth labels and predicted logits\n",
    "true_labels_numpy = np.array([1, 0, 2])  # Ground truth labels (indices)\n",
    "\n",
    "predicted_logits_numpy = np.array([[0.4, 1.5, 0.8],\n",
    "                                   [0.3, 0.9, 0.2],\n",
    "                                   [0.6, 0.1, 1.5]])  # Predicted logits\n",
    "\n",
    "# Apply softmax to convert logits into probabilities\n",
    "predicted_probs_numpy = softmax(predicted_logits_numpy)\n",
    "\n",
    "# Compute the categorical cross-entropy loss\n",
    "loss_numpy = categorical_cross_entropy(true_labels_numpy, predicted_probs_numpy)\n",
    "\n",
    "print('Categorical Cross-Entropy Loss with Numpy:', loss_numpy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
