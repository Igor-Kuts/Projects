{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7916f0e-99d6-4437-a34d-6de972b63af9",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "\n",
    "The **perceptron** is a fundamental building block in Machine Learning (ML) and artificial neural networks. It is a type of artificial neuron that takes multiple inputs, applies weights to those inputs, sums them up, and passes the result through an activation function to produce an output. \n",
    "\n",
    "The perceptron is a binary classifier, meaning it can classify inputs into two classes.\n",
    "***\n",
    "The perceptron was proposed by **Frank Rosenblatt** in 1957 and is one of the earliest models of an artificial neural network. It gained popularity due to its ability to learn and classify patterns, and it laid the foundation for many other artificial neural network models. It was inspired by the biological neurons found in the brain. \n",
    "\n",
    "![perceptron](https://andreyex.ru/wp-content/uploads/2019/07/TensorFlow-odnoslojnyj-perseptron_1.jpg)\n",
    "***\n",
    "#### Perceptron Convergence Theorem:\n",
    "\n",
    "The perceptron convergence theorem states that if a dataset is linearly separable, then the <u>perceptron learning algorithm is guaranteed to find a separating hyperplane in a finite number of steps</u>. This theorem provides theoretical support for the effectiveness of perceptrons in solving linear classification problems.\n",
    "\n",
    "## Basic components of Perceptron:\n",
    "\n",
    "The perceptron consists of several essential components:\n",
    "\n",
    "1. **Input Values**: The perceptron receives input values that represent features or attributes of the input data. Each input is assigned a weight, indicating its significance in influencing the perceptron's output.\n",
    "\n",
    "2. **Weights**: The weights associated with the inputs determine their relative importance. These weights can be positive or negative real numbers and play a crucial role in the perceptron's decision-making process.\n",
    "\n",
    "3. **Weighted Sum**: The perceptron calculates the weighted sum by multiplying each input value with its corresponding weight and then summing them up. This step represents the combination of inputs and weights.\n",
    "\n",
    "4. **Activation Function**: The weighted sum is passed through an activation function, which introduces non-linearity into the perceptron's output. The activation function helps determine whether the perceptron should activate or remain inactive based on the input it receives.\n",
    "\n",
    "5. **Threshold/Bias**: The perceptron can incorporate a bias term or threshold, which acts as an offset. It affects the decision boundary of the perceptron by adjusting the activation level required for the perceptron to fire.\n",
    "\n",
    "6. **Output**: The output of the perceptron is generated by applying the activation function to the weighted sum (plus bias, if present). It represents the perceptron's classification decision or prediction, such as assigning an input to a specific class or producing a binary output.\n",
    "\n",
    "By leveraging these fundamental components, the perceptron becomes capable of handling input data, performing a weighted summation, applying an activation function, and generating an output that facilitates classification or prediction tasks.\n",
    "\n",
    "Throughout the training process, the perceptron's weights and bias are adjusted iteratively using a learning algorithm and the provided training data. The objective is to determine the optimal values for these weights and bias, enabling the perceptron to effectively classify the training examples or minimize the discrepancy between the predicted and desired outputs.\n",
    "\n",
    "Through the combination of multiple perceptrons or the utilization of more intricate architectures like multi-layer perceptrons (MLPs), it becomes possible to tackle intricate problems that require non-linear decision boundaries.\n",
    "\n",
    "## Types of perceptrons:\n",
    "\n",
    "There are **two main types** of perceptrons: single-layer perceptrons and multi-layer perceptrons (also known as feedforward neural networks).\n",
    "\n",
    "- Single-layer perceptrons: These consist of a single layer of artificial neurons, which take inputs, compute a weighted sum, and apply an activation function. Single-layer perceptrons are limited to linearly separable problems, where data points can be divided into distinct classes by a straight line or hyperplane.\n",
    "\n",
    "- Multi-layer perceptrons (MLPs): These are composed of one or more hidden layers between the input and output layers. MLPs can learn and represent complex non-linear relationships, making them capable of solving more complicated problems.\n",
    "\n",
    "## Math behind perceptron:\n",
    "\n",
    "1. **Weighted sum of inputs with bias term**:\n",
    "\n",
    "   $$\n",
    "   weighted\\_sum = w_0 \\cdot b + w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\ldots + w_n \\cdot x_n\n",
    "   $$\n",
    "\n",
    "   where $w_0$ represents the bias weight, $b$ is the bias input (usually set to 1), and $x_i$ are the input features.\n",
    "\n",
    "2. **Activation function with threshold**:\n",
    "   $$\n",
    "   \\text{output} = \\begin{cases} \n",
    "                     1, & \\text{if }  weighted\\_sum \\geq \\text{threshold} \\\\\n",
    "                     0, & \\text{otherwise}\n",
    "                   \\end{cases}\n",
    "   $$\n",
    "   \n",
    "   The threshold can be a predetermined value, often set to 0.\n",
    "\n",
    "3. **Updating weights with learning rate**:\n",
    "\n",
    "   $$\n",
    "   w_i = w_i + \\text{{learning\\_rate}} \\times (y - \\text{{output}}) \\times x_i\n",
    "   $$\n",
    "\n",
    "   where $w_i$ is the weight for the $i$-th input, $y$ is the target output, and $x_i$ is the $i$-th input value.\n",
    "\n",
    "4. **Perceptron Loss function**:\n",
    "   $$\n",
    "   \\text{Loss} = \\frac{1}{N} \\sum_{i=1}^{N} (\\text{target}_i - \\text{output}_i)^2\n",
    "   $$\n",
    "   \n",
    "   This loss function represents the $\\text{Mean Squared Error (MSE)}$  between the target output $\\text{target}_i$ and the predicted output $\\text{output}_i$ of the perceptron for $N$ training examples.\n",
    "\n",
    "5. **Perceptron Update Rule**:\n",
    "   $$\n",
    "   \\Delta w_i = \\text{learning rate} \\times (\\text{target} - \\text{output}) \\times x_i\n",
    "   $$\n",
    "   \n",
    "   This formula represents the change in the weight $\\Delta w_i$ for the $i$-th input during each iteration of the training process.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a08bec-daab-4880-bc3a-78efc74cb65f",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b547a121-ce19-4d11-b734-c77e92bcbf38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acacdd7f-ec31-40d4-971e-cbaf6d27f7a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Single-layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fcadd31-8631-4cbc-b9ce-4d6660a8cb26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 0], Prediction: 0\n",
      "Input: [0 1], Prediction: 1\n",
      "Input: [1 0], Prediction: 1\n",
      "Input: [1 1], Prediction: 1\n"
     ]
    }
   ],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, num_features, learning_rate=0.1, num_epochs=100):\n",
    "        \"\"\"\n",
    "        Initialize the Perceptron classifier.\n",
    "\n",
    "        Args:\n",
    "            num_features (int): Number of input features.\n",
    "            learning_rate (float, optional): Learning rate for weight updates. Defaults to 0.1.\n",
    "            num_epochs (int, optional): Number of training epochs. Defaults to 100.\n",
    "        \"\"\"\n",
    "        self.num_features = num_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weights = np.zeros(num_features + 1)  # Additional weight for bias term\n",
    "\n",
    "    def activation(self, x):\n",
    "        \"\"\"\n",
    "        Activation function for the perceptron.\n",
    "\n",
    "        Args:\n",
    "            x (float): Input value.\n",
    "\n",
    "        Returns:\n",
    "            int: Output of the activation function (0 or 1).\n",
    "        \"\"\"\n",
    "        return 1 if x >= 0 else 0\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Perform prediction using the trained perceptron.\n",
    "\n",
    "        Args:\n",
    "            x (ndarray): Input features.\n",
    "\n",
    "        Returns:\n",
    "            int: Predicted class label (0 or 1).\n",
    "        \"\"\"\n",
    "        x_with_bias = np.insert(x, 0, 1)  # Add bias term\n",
    "        activation_input = np.dot(x_with_bias, self.weights)\n",
    "        return self.activation(activation_input)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the perceptron using the training data.\n",
    "\n",
    "        Args:\n",
    "            X (ndarray): Input features of shape (num_samples, num_features).\n",
    "            y (ndarray): Target labels of shape (num_samples,).\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the dimensions of X and y are inconsistent.\n",
    "        \"\"\"\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"Dimensions of X and y are inconsistent.\")\n",
    "\n",
    "        for _ in range(self.num_epochs):\n",
    "            for x, target in zip(X, y):\n",
    "                prediction = self.predict(x)\n",
    "                self.weights += self.learning_rate * (target - prediction) * np.insert(x, 0, 1)\n",
    "\n",
    "# Usage example with logical OR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input features for logical OR\n",
    "y = np.array([0, 1, 1, 1])  # Target labels for logical OR\n",
    "\n",
    "perceptron = Perceptron(num_features=2, learning_rate=0.1, num_epochs=100)\n",
    "perceptron.train(X, y)\n",
    "\n",
    "# Test the trained perceptron with logical OR inputs\n",
    "test_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "for x in test_data:\n",
    "    prediction = perceptron.predict(x)\n",
    "    print(f\"Input: {x}, Prediction: {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
