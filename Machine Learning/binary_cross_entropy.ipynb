{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd30eec-618c-48b7-a1c8-cd116ceccb62",
   "metadata": {},
   "source": [
    "# Binary Cross-Entropy Loss\n",
    "\n",
    "**Binary Cross-Entropy** is a commonly used loss function mostly used in binary classification problems.   \n",
    "It <u>measures the dissimilarity</u> between predicted probabilities and true binary labels.\n",
    "\n",
    "$$H_p(q) = - \\frac{1}{N}\\sum_{i=1}^{N}y_{i}\\cdot log(p(y_{i})) + (1 - y_{i}) \\cdot log(1 - p(y_{i}))$$\n",
    "\n",
    "In this formula, $N$ represents the number of instances in the dataset, $y_{i}$ is the true binary label for instance $i$, and $p(y_{i})$ is the predicted probability for instance $i$.\n",
    "\n",
    "The **BCE loss** is commonly used in machine learning frameworks like *PyTorch*, *TensorFlow*, and *scikit-learn*. It serves as an *objective* or *optimization function* during the training process, guiding the model to make accurate predictions and update its parameters to minimize the loss.  \n",
    "\n",
    "To train a binary classification model, the BCE loss is calculated for each data instance in the training set. The overall loss is obtained by taking the average or sum of the individual losses, depending on the implementation.\n",
    "\n",
    "By <font color=\"green\" size=\"2\">**minimizing the BCE loss**</b></font>, the model learns to <font color=\"orange\" size=\"2\"><b>assign higher probabilities to positive instances (belonging to the positive class) and lower probabilities to negative instances  (belonging to the negative class)</b></font>. This helps the model in making more accurate predictions on unseen data.\n",
    "\n",
    "Let's compute BCE Loss using different methods and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4697af3-2805-4cfb-875d-80ee6d06cc2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18454c0c-0a77-42d4-a9a6-d433065f572f",
   "metadata": {},
   "source": [
    "### Compute Binary Cross-Entropy Loss in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2318c9f-63af-4c23-9f3a-4f375937c87d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Below the `y_true` variable represents the true binary labels, and `y_pred` represents predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b73e5d26-cc90-47f9-b389-6f040b078539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simulated binary labels and predicted probabilities\n",
    "y_true = torch.tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0], dtype=torch.float32)  # True binary labels\n",
    "y_proba = torch.tensor([0.7, 0.2, 0.4, 0.3, 0.8, 0.6, 0.5, 0.9, 0.6, 0.2], dtype=torch.float32)  # Predicted probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d40735-3e7a-4df5-acd9-7a8d4ef20e1f",
   "metadata": {},
   "source": [
    "In practice, we would usually apply an activation function (such as sigmoid for binary classification tasks) to the model's final layer to obtain the probabilities between 0 and 1:\n",
    "```python\n",
    "# Applying sigmoid activation to obtain predicted probabilities\n",
    "y_proba = torch.sigmoid(y_proba)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f73074-2492-49e8-b3a0-68c9a036ba84",
   "metadata": {},
   "source": [
    "Next, we use PyTorch's `nn.BCELoss` class to compute the Binary Cross-Entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8de72db5-4dc0-4f45-a2fc-6c6a39c50401",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Cross-Entropy Loss: 0.5911\n"
     ]
    }
   ],
   "source": [
    "# Compute Binary Cross-Entropy loss\n",
    "bce_loss = nn.BCELoss()(y_proba, y_true)\n",
    "\n",
    "# Extract the loss value as scalar using .item() method\n",
    "pytorch_bce_loss = round(bce_loss.item(), 5)\n",
    "\n",
    "print(\"Binary Cross-Entropy Loss:\", pytorch_bce_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b257a016-12bb-440b-8539-c2d6c9cb536d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compute Binary Cross-Entropy Loss with SciKit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3aa12d-147b-4989-b9b8-548d7653e16d",
   "metadata": {},
   "source": [
    "Same as in the example above, `y_true` represents true binary labels and `y_proba` represents predicted probabilities, written as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cb99dd3-0a0e-4c3a-977b-4e2f7c8beeb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Binary Cross-Entropy Loss: tensor(0.5911)\n"
     ]
    }
   ],
   "source": [
    "# Simulated binary labels and predicted probabilities\n",
    "y_true = np.array([1, 0, 1, 0, 0, 1, 1, 1, 0, 0])\n",
    "y_proba = np.array([0.7, 0.2, 0.4, 0.3, 0.8, 0.6, 0.5, 0.9, 0.6, 0.2])\n",
    "\n",
    "# Compute Binary Cross-Entropy Loss via sklearn's log_loss\n",
    "sklearn_bce_loss = round(log_loss(y_true, y_proba), 5)\n",
    "print(\"Average Binary Cross-Entropy Loss:\", bce_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f2654b-5217-4d8c-898b-fc94fc87cf54",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compute Binary Cross-Entropy Loss with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41621f77-b938-4060-a9ee-da5b059c6ce4",
   "metadata": {},
   "source": [
    "Same as in the example above, `y_true` represents true binary labels and `y_proba` represents predicted probabilities, written as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e307ff1-d25b-49e9-969c-05ce161fcb76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simulated binary labels and predicted probabilities\n",
    "y_true = np.array([1, 0, 1, 0, 0, 1, 1, 1, 0, 0])\n",
    "y_proba = np.array([0.7, 0.2, 0.4, 0.3, 0.8, 0.6, 0.5, 0.9, 0.6, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b1eed8-4c95-4c3f-9b84-453f3ec2b1dd",
   "metadata": {},
   "source": [
    "Again, in practice we would have to apply an activation function (such as sigmoid) to the model's final layer to obtain the probabilities between 0 and 1.   \n",
    "\n",
    "For binary classification tasts this can be achieved via applying simple sigmoid function.\n",
    "\n",
    "```python\n",
    "def sigmoid(arr):\n",
    "    \"\"\"\n",
    "    Apply the sigmoid activation function element-wise to an array.\n",
    "\n",
    "    Parameters:\n",
    "        arr (numpy.ndarray): Array of numeric values.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array with sigmoid activation applied element-wise.\n",
    "    \"\"\"\n",
    "    return 1 / (np.ones(len(arr)) + np.exp(-arr))\n",
    "\n",
    "y_proba = sigmoid(y_proba)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63aec54-a9c4-4f6c-b003-e30130e45259",
   "metadata": {},
   "source": [
    "Let's define a custom function to compute Binary Cross-Entropy Loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9951e8f-4c74-4dd3-831e-506d21ef99a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute Binary Cross-Entropy (BCE) loss.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (numpy.ndarray): Array of true binary labels (0 or 1).\n",
    "        y_pred (numpy.ndarray): Array of predicted probabilities between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array of Binary Cross-Entropy (BCE) loss values.\n",
    "    \"\"\"\n",
    "    bce_loss = -(y_true * np.log(y_pred) + (np.ones(len(y_true)) - y_true) * np.log(np.ones(len(y_pred)) - y_pred))\n",
    "    \n",
    "    return bce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88d3c896-4504-4bc6-a5c2-da1059f89bad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Binary Cross-Entropy Loss: 0.5911\n"
     ]
    }
   ],
   "source": [
    "# Compute Binary Cross-Entropy Loss element wise\n",
    "bce_loss = binary_cross_entropy(y_true, y_proba)\n",
    "\n",
    "# Compute average Binary Cross-Entropy Loss\n",
    "numpy_bce_loss = round(np.mean(bce_loss), 5)\n",
    "\n",
    "print(\"Average Binary Cross-Entropy Loss:\", numpy_bce_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beb8e76-68ab-4703-9da3-cde4f9b3a69f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Comparison of the BCE computation results between PyTorch, Sci-kit Learn and Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3b8032-8bce-4027-b22d-696c7726c0c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "Recall that we have previously saved Binary Cross-Entropy computation results rounded to 5 decimal numbers as:  \n",
    "`pytorch_bce_loss`, `sklearn_bce_loss` and `numpy_bce_loss` variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "653c7fcf-58ee-4951-a143-b8399136469c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for equality:\n",
    "pytorch_bce_loss == sklearn_bce_loss == numpy_bce_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
