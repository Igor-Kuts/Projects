{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34529586-486c-40ac-bf06-2508780ea629",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction to vector distances\n",
    "\n",
    "In <span style=\"font-size: 11pt; color: steelblue; font-weight: bold\">Data Analysis</span> and <span style=\"font-size: 11pt; color: steelblue; font-weight: bold\">Machine Learning</span>, it is <u>crucial to understand how to measure the distance or dissimilarity between vectors</u>.  \n",
    "\n",
    "Various distance metrics exist, each with its own strengths and limitations. This short piece of work provides an overview of commonly used vector distance measures, their pros and cons, way of computation and applications.  \n",
    "\n",
    "By exploring these distances, we will gain a deeper understanding of their importance in Data Analysis and Machine Learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4135d03f-c902-4b23-9fa9-0575d82f7fc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Vector Distances\n",
    "\n",
    "## <span style=\"font-size: 18pt; color: goldenrod; font-weight: bold\">Euclidean Distance</span>:\n",
    "#### Overview:\n",
    "Euclidean distance or **L2 Distance** measures the <u>straight-line distance between two points in a multi-dimensional space</u>. It calculates the length of the line segment connecting the two points.\n",
    "\n",
    "#### Formula:\n",
    "For two vectors, $A$ and $B$, each with $n$ dimensions, the Euclidean distance is computed as:\n",
    "\n",
    "$$\\sqrt{\\sum_{i=1}^{n}(A_i-B_i)^2})$$\n",
    "\n",
    "#### Pros and Cons:\n",
    "| Pros                           | Cons                               |\n",
    "|--------------------------------|------------------------------------|\n",
    "| Intuitive interpretation       | Sensitive to outliers              |\n",
    "| Captures differences in all dimensions | Doesn't handle sparse data well |\n",
    "| Widely used in various algorithms |                                 |\n",
    "\n",
    "\n",
    "#### Trivia:\n",
    "   - The Euclidean distance is a special case of the Minkowski distance with p=2.\n",
    "\n",
    "#### Computation in Python:\n",
    "```python\n",
    "import numpy as np\n",
    "   \n",
    "def euclidean_distance(A, B):\n",
    "    return np.sqrt(np.sum((A - B) ** 2))\n",
    "```\n",
    "***\n",
    "## <span style=\"font-size: 18pt; color: goldenrod; font-weight: bold\">Manhattan Distance</span>:\n",
    "#### Overview:\n",
    "Manhattan distance, also known as the \"city block\" distance or **L1 Distance**, measures the distance between two points by summing the absolute differences of their corresponding components.\n",
    "\n",
    "#### Formula:\n",
    "For two vectors, $A$ and $B$, with $n$ dimensions, the Manhattan distance is calculated as:\n",
    "\n",
    "$$\\sum_{i=1}^{n}|A_i-B_i|$$\n",
    "\n",
    "#### Pros and Cons:\n",
    "\n",
    "| Pros                                        | Cons                                        |\n",
    "|---------------------------------------------|---------------------------------------------|\n",
    "| Intuitive interpretation as the sum of absolute differences.|Ignores differences in magnitude across dimensions|\n",
    "|Robust to outliers. | Not suitable for continuous data.|\n",
    "|Suitable for grid-like structures.|\n",
    "\n",
    "\n",
    "#### Trivia:\n",
    "   - The Manhattan distance is a special case of the Minkowski distance with p=1.\n",
    "\n",
    "#### Computation in Python:\n",
    "   ```python\n",
    "   import numpy as np\n",
    "   \n",
    "   def manhattan_distance(A, B):\n",
    "       return np.sum(np.abs(A - B))\n",
    "   ```\n",
    "***\n",
    "## <span style=\"font-size: 18pt; color: goldenrod; font-weight: bold\">Cosine Similarity</span>:\n",
    "#### Overview:\n",
    "   Cosine similarity measures the cosine of the angle between two vectors. It is often <span style=\"font-size: 11pt; color: seagreen; font-weight: bold\">used to determine the similarity between documents or high-dimensional vectors</span>.\n",
    "\n",
    "#### Formula:\n",
    "   For two vectors, $A$ and $B$, the cosine similarity is computed as:\n",
    "\n",
    "   $$\\frac{{A \\cdot B}}{{\\|A\\| \\cdot \\|B\\|}}$$\n",
    "where $\\|A\\|$ stands for vector norm / magnitude / length\n",
    "\n",
    "For a vector, such as $A$, the norm $\\|A\\|$ represents the length or magnitude of the vector. The specific norm used (e.g., Euclidean norm or L2 norm) will depend on the context and the mathematical definition being used.\n",
    "\n",
    "For example, the Euclidean norm or **L2 norm** of a vector $A = (a_1, a_2, \\ldots, a_n)$ in $n$-dimensional space is calculated as:\n",
    "\n",
    "$$\n",
    "\\|A\\| = \\sqrt{a_1^2 + a_2^2 + \\ldots + a_n^2}\n",
    "$$\n",
    "\n",
    "Similarly, for matrices or other mathematical objects, the notation $\\|A\\|$ represents the norm or magnitude associated with that object, which again will depend on the specific context and definition being used.\n",
    "\n",
    "#### Pros and Cons:\n",
    "\n",
    "| Pros                                          | Cons                                        |\n",
    "|-----------------------------------------------|---------------------------------------------|\n",
    "| Independent of vector magnitude.              | Doesn't capture magnitude differences.       |\n",
    "| Measures the similarity rather than distance. | Can be affected by vectors of all zeros.     |\n",
    "| Effective for high-dimensional and sparse data.|                                             |\n",
    "\n",
    "#### Trivia:\n",
    "   - The cosine similarity ranges from -1 to 1, where <u>1 represents identical vectors, 0 indicates orthogonality, and -1 indicates complete dissimilarity</u>.\n",
    "\n",
    "#### Computation in Python:\n",
    "   ```python\n",
    "   import numpy as np\n",
    "   \n",
    "   def cosine_similarity(A, B):\n",
    "       dot_product = np.dot(A, B)\n",
    "       norm_A = np.linalg.norm(A)\n",
    "       norm_B = np.linalg.norm(B)\n",
    "       return dot_product / (norm_A * norm_B)\n",
    "   ```\n",
    "***\n",
    "## <span style=\"font-size: 18pt; color: goldenrod; font-weight: bold\">Minkowski Distance</span>:\n",
    "#### Overview:\n",
    "   The Minkowski distance is a generalization of both the Euclidean and Manhattan distances. It allows adjusting the sensitivity to different dimensions using a parameter called the \"**order**\" or \"**p**\".\n",
    "\n",
    "#### Formula:\n",
    "For two vectors, $A$ and $B$, the Minkowski distance of order $p$ is computed as:\n",
    "\n",
    "$$\\sqrt[p]{\\sum_{i=1}^{n}|A_i-B_i|^p}$$\n",
    "\n",
    "#### Pros and Cons:\n",
    "\n",
    "| Pros                           | Cons                               |\n",
    "|--------------------------------|------------------------------------|\n",
    "|Flexible distance metric that encompasses both Euclidean and Manhattan distances.|Selection of the parameter p can be challenging.|\n",
    "|Allows adjusting sensitivity to different dimensions.|Sensitive to outliers, especially with high values of p.|\n",
    "|| \n",
    "\n",
    "#### Trivia:\n",
    "   - The Manhattan distance corresponds to the Minkowski distance with p=1, and the Euclidean distance corresponds to p=2.\n",
    "\n",
    "#### Computation in Python:\n",
    "   ```python\n",
    "   import numpy as np\n",
    "   \n",
    "   def minkowski_distance(A, B, p):\n",
    "       return np.power(np.sum(np.abs(A - B) ** p), 1/p)\n",
    "   ```\n",
    "***\n",
    "## <span style=\"font-size: 18pt; color: goldenrod; font-weight: bold\">Hamming Distance</span>:\n",
    "#### Overview:\n",
    "   Hamming distance is primarily used for comparing vectors of equal length that represent binary strings. It measures the number of positions at which the corresponding elements between two vectors differ.\n",
    "\n",
    "#### Formula:\n",
    "   For two binary vectors, $A$ and $B$, the Hamming distance is calculated as the count of differing elements.\n",
    "\n",
    "#### Pros and Cons:\n",
    "| Pros                                             | Cons                                        |\n",
    "|--------------------------------------------------|---------------------------------------------|\n",
    "| Effective for categorical or binary data.| Limited to binary or categorical data.      |\n",
    "| Useful for error detection and correction.| Not suitable for continuous or numerical data.|\n",
    "\n",
    "\n",
    "#### Trivia:\n",
    "   - The Hamming distance is equivalent to the Manhattan distance for binary vectors.\n",
    "\n",
    "#### Computation in Python:\n",
    "   ```python\n",
    "   def hamming_distance(A, B):\n",
    "       return np.count_nonzero(A != B)\n",
    "   ```\n",
    "---\n",
    "## <span style=\"font-size: 18pt; color: goldenrod; font-weight: bold\">Mahalanobis Distance</span>:\n",
    "#### Overview:\n",
    "   The Mahalanobis distance measures the distance between a <u>point</u> and a <u>distribution</u>, taking into account the covariance structure of the data. It considers the variability and correlation of the data.\n",
    "\n",
    "#### Formula:\n",
    "   For two vectors, A and B, the Mahalanobis distance is computed as:\n",
    "\n",
    "$$\\sqrt{{(A-B)^\\top\\Sigma^{-1}(A-B)}}$$\n",
    "\n",
    "#### Pros and Cons:\n",
    "\n",
    "| Pros                                                       | Cons                                                        |\n",
    "| ---------------------------------------------------------- | ----------------------------------------------------------- |\n",
    "| Accounts for covariance structure                           | Requires estimation of the covariance matrix                |\n",
    "| Useful for high-dimensional data                            | Sensitive to assumptions about the data distribution       |\n",
    "| Handles outliers better than Euclidean distance             |                                                             |\n",
    "\n",
    "#### Trivia:\n",
    "   - The Mahalanobis distance can be used for <span style=\"font-size: 11pt; color: seagreen; font-weight: normal\">**outlier detection and clustering analysis**</span>.\n",
    "\n",
    "#### Computation in Python:\n",
    "   ```python\n",
    "   import numpy as np\n",
    "   \n",
    "   def mahalanobis_distance(A, B, covariance_matrix):\n",
    "       diff = A - B\n",
    "       inverse_cov = np.linalg.inv(covariance_matrix)\n",
    "       return np.sqrt(np.dot(np.dot(diff, inverse_cov), diff.T))\n",
    "   ```\n",
    "***\n",
    "## <span style=\"font-size: 18pt; color: goldenrod; font-weight: bold\">Jaccard Distance</span>:\n",
    "#### Overview:\n",
    "   The Jaccard distance measures the dissimilarity between sets or binary vectors. It quantifies the difference in terms of the size of the symmetric difference of the sets divided by the size of their union.\n",
    "\n",
    "#### Formula:\n",
    "   For two sets or binary vectors, A and B, the Jaccard distance is calculated as:\n",
    "\n",
    "$$\\frac{{|A \\cup B|-|A \\cap B|}}{{|A \\cup B|}}$$\n",
    "\n",
    "#### Pros and Cons:\n",
    "Pros | Cons\n",
    "--- | ---\n",
    "Suitable for comparing sets or binary data. | Limited to binary or categorical data.\n",
    "Measures the dissimilarity rather than distance. | Not applicable for continuous or numerical data.\n",
    "Effective for data with varying lengths. | \n",
    "\n",
    "#### Trivia:\n",
    "   - The Jaccard distance is used in applications like <span style=\"font-size: 11pt; color: seagreen; font-weight: normal\">**text mining, recommendation systems, and clustering.**</span>\n",
    "\n",
    "#### Computation in Python:\n",
    "   ```python\n",
    "   def jaccard_distance(A, B):\n",
    "       union = np.union1d(A, B)\n",
    "       intersection = np.intersect1d(A, B)\n",
    "       return 1 - len(intersection) / len(union)\n",
    "   ```\n",
    "***\n",
    "## <span style=\"font-size: 18pt; color: goldenrod; font-weight: bold\">Chebyshev Distance</span>:\n",
    "#### Overview:\n",
    "The Chebyshev distance is a metric used to measure dissimilarity or distance between two points in a vector space. It calculates the maximum absolute difference between the corresponding elements of two vectors. In other words, it measures the maximum distance between any dimension of the two vectors.\n",
    "\n",
    "#### Formula:\n",
    "\n",
    "The Chebyshev distance between two vectors, $A = (a₁, a₂, ..., aₙ)$ and $B = (b₁, b₂, ..., bₙ)$, is calculated as:\n",
    "\n",
    "$$\\max\\left(|a₁ - b₁|, |a₂ - b₂|, \\ldots, |aₙ - bₙ|\\right)$$\n",
    "\n",
    "#### Pros and Cons:\n",
    "\n",
    "| Pros                                                  | Cons                                                     |\n",
    "| ----------------------------------------------------- | -------------------------------------------------------- |\n",
    "| Suitable for comparing sets or binary data             | Limited to binary or categorical data                     |\n",
    "| Measures the dissimilarity rather than distance        | Not applicable for continuous or numerical data           |\n",
    "| Effective for data with varying lengths                |                                                          |\n",
    "\n",
    "#### Computation in Python:\n",
    "```python\n",
    "chebyshev_distance = np.max(np.abs(A - B))\n",
    "```\n",
    "\n",
    "#### Trivia:\n",
    "The Chebyshev distance is named after the Russian mathematician Pafnuty Chebyshev. It is commonly used in applications such as <span style=\"font-size: 11pt; color: seagreen; font-weight: normal\">**pattern recognition, computer vision, and outlier detection**</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b376caa1-8534-4868-98da-5bab082552a8",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "<span style=\"font-size: 16pt; color: steelblue; font-weight: bold\">Understanding vector distances is crucial for various Data Analysis and Machine Learning tasks.</span>  \n",
    "\n",
    "By grasping the strengths and limitations of different distance measures like **Euclidean**, **Manhattan**, **cosine similarity**, **Minkowski**, **Hamming**, **Mahalanobis**, and **Jaccard distances**, we gain valuable tools to <u>assess the similarities or dissimilarities</u> between vectors. \n",
    "\n",
    "<u>Each distance metric has its own unique characteristics</u>, making them suitable for specific scenarios. \n",
    "\n",
    "<span style=\"font-size: 11pt; color: goldenrod; font-weight: bold\">Expanding our knowledge of vector distances equips us with the ability to choose the most appropriate distance measure for our data analysis needs.</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
