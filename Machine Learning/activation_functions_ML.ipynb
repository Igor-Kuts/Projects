{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0681755-5870-4682-a202-57e81173d3a1",
   "metadata": {},
   "source": [
    "***\n",
    "**Disclaimer**: This notebook is lenghty, <u>navigate the links given below, if need be</u>.\n",
    "\n",
    "## [Sigmoid](#sigmoid)\n",
    "## [ReLU](#relu)\n",
    "## [Leaky ReLU](#leaky_relu)\n",
    "## [History](#history)\n",
    "## [Landmarks of CNN in CV](#landmarks)\n",
    "\n",
    "**In the second cell of this notebook, we will create a custom function to compute a 3D convolution using Numpy library and FOR LOOPS to iterate over the input.**\n",
    "\n",
    "It's worth noting that **there are more efficient and optimized ways to implement convolutions** using libraries like <span style=\"font-size: 11pt; color: orange; font-weight: normal\">**NumPy**</span> or frameworks like <span style=\"font-size: 11pt; color: orange; font-weight: normal\">**TensorFlow**</span> or <span style=\"font-size: 11pt; color: orange; font-weight: normal\">**PyTorch**</span>. These libraries provide highly optimized functions and operations for convolutions, taking advantage of vectorized computations and parallel processing.\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed93853-bf51-47c5-a9d5-313f7ca3222c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a id=\"sigmoid\"></a>Sigmoid:\n",
    "\n",
    "The sigmoid function maps the input to a range between 0 and 1, making it suitable for binary classification problems or as the output activation in multi-class classification.\n",
    "\n",
    "Sigmoid \"squashes\" the input to the range (0, 1) using the logistic function.\n",
    "\n",
    "Sigmoid suffers from the vanishing gradient problem, where gradients become close to zero, and saturation for large inputs, making it challenging to train deep neural networks.\n",
    "\n",
    "**Application**: Sigmoid is commonly used in <u>binary classification tasks</u>.\n",
    "\n",
    "$$\\text{sigmoid}(x) = \\frac{1}{1 + \\exp^{(-x)}}$$\n",
    "\n",
    "```python\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "tf.sigmoid(x)\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "torch.sigmoid(x)\n",
    "\n",
    "# NumPy implementation:\n",
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "```\n",
    "\n",
    "## <a id=\"relu\"></a>ReLU (Rectified Linear Unit):\n",
    "    \n",
    "ReLU returns the input as is if it is positive, and 0 otherwise. ReLU is widely used as the activation function in hidden layers of deep neural networks, especially in convolutional neural networks (CNNs)\n",
    "    \n",
    "ReLU is computationally efficient, as it only involves simple thresholding operations. It overcomes the vanishing gradient problem and accelerates convergence in deep neural networks. It has been successful in training deep architectures and achieves good generalization.\n",
    "    \n",
    "**NOTE**: ReLU can suffer from the \"dying ReLU\" problem, where neurons become permanently inactive if their output falls below zero. It can cause dead neurons and a significant number of zero-valued gradients during backpropagation.\n",
    "    \n",
    "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "\n",
    "```python\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "tf.nn.relu(x)\n",
    "\n",
    "# PyTorch code:\n",
    "import torch\n",
    "torch.relu(x)\n",
    "\n",
    "# NumPy implementation:\n",
    "import numpy as np\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "```\n",
    "\n",
    "## <a id=\"leaky_relu\"></a>Leaky ReLU:\n",
    "    \n",
    "Leaky ReLU is a variation of ReLU that introduces a small slope for negative values, addressing the \"dying ReLU\" problem where neurons become inactive during training, thus allowing gradients to flow. It helps prevent dead neurons and can improve model performance compared to standard ReLU\n",
    "    \n",
    "Leaky ReLU is used as an alternative to ReLU, especially in models where the \"dying ReLU\" problem is prevalent. It can be beneficial in scenarios where standard ReLU fails to provide satisfactory results.\n",
    "\n",
    "**NOTE**: Leaky ReLU introduces additional hyperparameters, making it more complex to tune. The choice of the slope parameter can affect the model's performance.\n",
    "    \n",
    "Leaky ReLU sets positive values unchanged and negative values with a small slope, typically a small fraction like 0.01\n",
    "    \n",
    "$$\\text{LeakyReLU}(x) = \\begin{cases} x, & \\text{if } x \\geq 0 \\\\ \\alpha \\cdot x, & \\text{if } x < 0 \\end{cases}$$\n",
    "    \n",
    "```python\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "tf.nn.leaky_relu(x, alpha)\n",
    "\n",
    "#PyTorch:\n",
    "import torch\n",
    "torch.nn.functional.leaky_relu(x, negative_slope=alpha)\n",
    "\n",
    "#NumPy implementation:\n",
    "import numpy as np\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x >= 0, x, alpha * x)\n",
    "```\n",
    "\n",
    "## Softmax:\n",
    "    \n",
    "Softmax is commonly used in the final (output) layer for multi-class classification. It normalizes the outputs into a probability distribution that sums up to 1, enabling class predictions. \n",
    "    \n",
    "Softmax computes the exponential of each input element and normalizes them to obtain a probability distribution. It provides a clear interpretation of the model's confidence in each class prediction\n",
    "    \n",
    "*NOTE**: Softmax is sensitive to outliers and can amplify the differences between input values, potentially leading to numerical instability. It does not handle class imbalance well.\n",
    "    \n",
    "$$\\text{softmax}(x_i) = \\frac{\\exp(x_i)}{\\sum_{j=1}^{K}\\exp(x_j)}$$ for $i = 1, 2, \\ldots, K$, where $K$ is the number of classes.\n",
    "    \n",
    "\n",
    "```python\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "tf.nn.softmax(x)\n",
    "\n",
    "# PyTorch code:\n",
    "import torch\n",
    "torch.nn.functional.softmax(x, dim=1)\n",
    "\n",
    "# NumPy implementation:\n",
    "import numpy as np\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "```\n",
    "\n",
    "\n",
    "## Tanh:\n",
    "The hyperbolic tangent function, $tanh$, maps the input to a range between -1 and 1. It is commonly used in the hidden layers of neural networks.\n",
    "    \n",
    "Tanh is an S-shaped function that squashes the input to the range (-1, 1).\n",
    "\n",
    "Tanh produces values between -1 and 1, which helps in capturing negative correlations in the data. It is zero-centered, making it useful in the hidden layers of neural networks\n",
    "    \n",
    "Tanh is often used as an activation function in recurrent neural networks (RNNs), autoencoders, and models where negative correlations in the data are important.\n",
    "    \n",
    "**NOTE**: Tanh shares some drawbacks with sigmoid, such as the vanishing gradient problem and saturation for large inputs. It is prone to gradients close to zero in the saturated regions, slowing down training\n",
    "    \n",
    "$$\\text{tanh}(x) = \\frac{{\\exp(x) - \\exp(-x)}}{{\\exp(x) + \\exp(-x)}}$$\n",
    "\n",
    "```python\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.nn.tanh(x)\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "torch.tanh(x)\n",
    "\n",
    "# NumPy\n",
    "import numpy as np\n",
    "\n",
    "np.tanh(x)\n",
    "```\n",
    "\n",
    "## Linear\n",
    "    \n",
    "The linear activation function, also known as the identity function, returns the input as is without any non-linear transformation. It is typically used in regression models or when a neural network needs to output a continuous value.\n",
    "\n",
    "Linear activation function (identity) preserves the input as is, making it suitable for regression tasks or when a neural network needs to output a continuous value without non-linear transformation.\n",
    "    \n",
    "**NOTE**: Linear activation lacks non-linearity, limiting the representational power of the neural network. It cannot model complex relationships between inputs and outputs.\n",
    "    \n",
    "$$\\text{linear}(x) = x$$\n",
    "\n",
    "```python\n",
    "def linear(x):\n",
    "    return x\n",
    "```\n",
    "\n",
    "## ELU (Exponential Linear Units):\n",
    "    \n",
    "ELU is an activation function that addresses the vanishing gradient problem while allowing negative values. It provides smooth outputs for negative values and encourages the network to have mean activations close to zero.   \n",
    "    \n",
    "**NOTE**: ELU introduces additional computation due to the exponential function, which can slightly slow down the training processand an additional hyperparameter which has to be tuned.   \n",
    "    \n",
    "ELU is similar to ReLU for positive values but uses an exponential function for negative values, controlled by the parameter $\\alpha$.\n",
    "    \n",
    "$$\\text{ELU}(x) = \\begin{cases} x, & \\text{if } x \\geq 0 \\\\\n",
    "\\alpha \\cdot (\\exp(x) - 1), & \\text{if } x < 0 \\end{cases}$$\n",
    "    \n",
    "```python\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "tf.nn.elu(x)\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "torch.nn.functional.elu(x, alpha=alpha)\n",
    "\n",
    "# NumPy implementation:\n",
    "import numpy as np\n",
    "\n",
    "def elu(x, alpha=1.0):\n",
    "    return np.where(x >= 0, x, alpha * (np.exp(x) - 1))\n",
    "```\n",
    "\n",
    "## PReLU (Parametric Rectified Linear Units):\n",
    "    \n",
    "PReLU is an extension of ReLU that introduces a learnable parameter $\\alpha$ for the negative slope. It provides flexibility and adaptability to the data during training, potentially improving model performance.\n",
    "\n",
    "PReLU is useful in scenarios where ReLU fails to capture the non-linear relationships in the data. It has been applied to various deep learning models, including CNNs and neural machine translation.    \n",
    "    \n",
    "**NOTE**: PReLU increases the model's complexity due to the additional learnable parameter. It requires careful initialization and can be prone to overfitting.\n",
    "\n",
    "$$\\text{PReLU}(x) = \\begin{cases} x, & \\text{if } x \\geq 0 \\\\ \\alpha \\cdot x, & \\text{if } x < 0 \\end{cases}$$\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# While main body of function remains the same ...\n",
    "def prelu(x, alpha=0.01):\n",
    "    return np.maximum(0.0, x) + alpha * np.minimum(0.0, x)\n",
    "\n",
    "# Implementation of learnable parameter << ALPHA >> for PReLU,\n",
    "# Unfortunately, is out of the scope of this article\n",
    "```\n",
    "\n",
    "## GELU (Gaussian Error Linear Units):\n",
    "    \n",
    "GELU is an activation function that approximates the cumulative distribution function (CDF) of a Gaussian distribution.\n",
    "\n",
    "GELU has been particularly successful in transformer-based architectures, such as the Transformer models for machine translation, language understanding, and other natural language processing tasks. It has shown improved performance compared to traditional activation functions like ReLU in such scenarios.\n",
    "    \n",
    "GELU smoothly saturates positive and negative values using a Gaussian cumulative distribution approximation.\n",
    "    \n",
    "$$\\text{GELU}(x) = 0.5 \\cdot \\left(1 + \\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right)$$\n",
    "\n",
    "\n",
    "\n",
    "The erf (error function) is a special function that cannot be expressed in terms of elementary functions like polynomials, exponentials, or trigonometric functions. It is commonly denoted as $\\text{erf}(x)$ and is defined as follows:\n",
    "\n",
    "$$ \\text{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^2} dt $$\n",
    "\n",
    "The error function is an odd function, symmetric around the origin, with a range from -1 to 1. It is primarily used in statistics and probability calculations involving normal distributions, such as calculating cumulative probabilities, quantiles, or the complementary error function (erfc).\n",
    "\n",
    "In the context of the GELU activation function, the erf function is used to approximate the cumulative distribution function (CDF) of a Gaussian distribution, allowing GELU to provide a smooth non-linear behavior.   \n",
    "    \n",
    "```python\n",
    "# Unfortunately, due to complexity,\n",
    "# implementation of GELU is out of the scope of this article\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
